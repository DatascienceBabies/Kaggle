{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "df_train_y = df['Survived'].values\n",
    "df_train_x = df.drop(['Survived'], axis=1)\n",
    "df_test_x = pd.read_csv('test.csv')\n",
    "\n",
    "test_passenger_ids = df_test_x['PassengerId']\n",
    "test_passenger_ids = np.reshape(test_passenger_ids.values, (df_test_x.shape[0], 1))\n",
    "\n",
    "labels = df_train_x.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove unused Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_unused_cols(dataset, labels):\n",
    "    unused_cols = [\"PassengerId\", \"Name\", \"Cabin\", \"Embarked\", \"Ticket\"]\n",
    "    \n",
    "    for col_name in unused_cols:\n",
    "        labels = labels[labels != col_name]\n",
    "        dataset = dataset.drop([col_name], axis=1)\n",
    "       \n",
    "    \n",
    "    return dataset, labels\n",
    "\n",
    "df_train_x, labels = remove_unused_cols(df_train_x, labels)\n",
    "df_test_x, _ = remove_unused_cols(df_test_x, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "**Fill NaN Values for**\n",
    "- Age\n",
    "\n",
    "**Apply One Hot Encoding on**\n",
    "- Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we check for sibling/parents and use this as a criteria for the random value?\n",
    "def fill_missing_age_fields(dataset):\n",
    "    dataset[\"Age\"].fillna(random.choice(dataset[dataset[\"Age\"] != np.nan][\"Age\"]), inplace =True)\n",
    "    \n",
    "fill_missing_age_fields(df_train_x)\n",
    "fill_missing_age_fields(df_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_column(dataset, col_names):\n",
    "    for col_name in col_names:\n",
    "        col = dataset.loc[:, col_name]\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_encoded = label_encoder.fit_transform(col.values)\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "        onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "        dataset = dataset.drop([col_name], axis=1)\n",
    "\n",
    "        for i in range(0, onehot_encoded.shape[1]):\n",
    "            new_col_name = \"{0}_{1}\".format(col_name, str(i))\n",
    "            dataset[new_col_name] = onehot_encoded[:, i]\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "columns_to_one_hot_encode = ['Pclass', 'Sex']\n",
    "\n",
    "df_train_x = one_hot_encode_column(df_train_x, columns_to_one_hot_encode)\n",
    "df_test_x = one_hot_encode_column(df_test_x, columns_to_one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(df_train_x)\n",
    "train_x = scaler.transform(df_train_x)\n",
    "test_x = scaler.transform(df_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Magic\n",
    "\n",
    "**Todo**:\n",
    "- k-Fold Cross Validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define binary classification model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=train_x.shape[1]))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "891/891 [==============================] - 0s 504us/step - loss: 0.6409 - acc: 0.6049\n",
      "Epoch 2/20\n",
      "891/891 [==============================] - 0s 100us/step - loss: 0.5447 - acc: 0.8002\n",
      "Epoch 3/20\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4913 - acc: 0.8047\n",
      "Epoch 4/20\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4604 - acc: 0.8092\n",
      "Epoch 5/20\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4448 - acc: 0.8103\n",
      "Epoch 6/20\n",
      "891/891 [==============================] - 0s 106us/step - loss: 0.4366 - acc: 0.8126\n",
      "Epoch 7/20\n",
      "891/891 [==============================] - 0s 107us/step - loss: 0.4313 - acc: 0.8092\n",
      "Epoch 8/20\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4276 - acc: 0.8103\n",
      "Epoch 9/20\n",
      "891/891 [==============================] - 0s 101us/step - loss: 0.4245 - acc: 0.8148\n",
      "Epoch 10/20\n",
      "891/891 [==============================] - 0s 104us/step - loss: 0.4217 - acc: 0.8171\n",
      "Epoch 11/20\n",
      "891/891 [==============================] - 0s 98us/step - loss: 0.4194 - acc: 0.8148\n",
      "Epoch 12/20\n",
      "891/891 [==============================] - 0s 102us/step - loss: 0.4178 - acc: 0.8193\n",
      "Epoch 13/20\n",
      "891/891 [==============================] - 0s 104us/step - loss: 0.4159 - acc: 0.8182\n",
      "Epoch 14/20\n",
      "891/891 [==============================] - 0s 110us/step - loss: 0.4143 - acc: 0.8171\n",
      "Epoch 15/20\n",
      "891/891 [==============================] - 0s 111us/step - loss: 0.4135 - acc: 0.8182\n",
      "Epoch 16/20\n",
      "891/891 [==============================] - 0s 119us/step - loss: 0.4119 - acc: 0.8215\n",
      "Epoch 17/20\n",
      "891/891 [==============================] - 0s 117us/step - loss: 0.4105 - acc: 0.8227\n",
      "Epoch 18/20\n",
      "891/891 [==============================] - 0s 139us/step - loss: 0.4097 - acc: 0.8215\n",
      "Epoch 19/20\n",
      "891/891 [==============================] - 0s 129us/step - loss: 0.4088 - acc: 0.8227\n",
      "Epoch 20/20\n",
      "891/891 [==============================] - 0s 122us/step - loss: 0.4075 - acc: 0.8249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea43ce7eb8>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(train_x, train_y, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model against the test data\n",
    "predict_y = model.predict(test_x)\n",
    "predict_y = np.around(predict_y)\n",
    "predict_y = predict_y.astype(np.integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write our predictions to a csv file\n",
    "csv_predict = np.concatenate((test_passenger_ids, predict_y), axis=1)\n",
    "csv_predict = np.concatenate((np.reshape([\"PassengerId\", \"Survived\"], (1, 2)), csv_predict))\n",
    "with open('prediction.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerows(csv_predict)\n",
    "csvFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
